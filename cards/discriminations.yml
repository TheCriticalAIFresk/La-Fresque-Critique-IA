card_number: 6
card_set: 2
title_en: Discriminations
description_en: The use of AI in sensitive areas (justice, employment, health, security) can perpetuate and amplify systemic oppressions, exposing gender minorities, racialized people and other marginalized groups to various forms of exclusion.
title_fr: Discriminations
description_fr: L’usage de l’IA dans des domaines sensibles (justice, emploi, santé, sécurité) peut perpétuer et amplifier les discriminations systémiques, exposant minorités de genre, personnes racisées et autres groupes marginalisés à des formes d’exclusion.
img: "imgs/discriminations.jpg"
bibliography:
  - "Capraro, V., Lentsch, A., Acemoglu, D., Akgun, S., Akhmedova, A., Bilancini, E., Bonnefon, J.-F., Brañas-Garza, P., Butera, L., Douglas, K. M., Everett, J. A. C., Gigerenzer, G., Greenhow, C., Hashimoto, D. A., Holt-Lunstad, J., Jetten, J., Johnson, S., Kunz, W. H., Longoni, C., … Viale, R. (2024). The impact of generative artificial intelligence on socioeconomic inequalities and policy making. PNAS Nexus, 3(6), page 191. https://doi.org/10.1093/pnasnexus/pgae191"
  - "Valdivia, A., & Tazzioli, M. (2023). Datafication Genealogies beyond Algorithmic Fairness : Making Up Racialised Subjects. Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency, 840‑850. https://doi.org/10.1145/3593013.3594047"
  - "De Roock, R. S. (2024). To Become an Object Among Objects : Generative Artificial “Intelligence,” Writing, and Linguistic White Supremacy. Reading Research Quarterly, 59(4), 590‑608. https://doi.org/10.1002/rrq.569"
  - "Amaro, Ramon. 2022. The Black Technical Object: On Machine Learning and the Aspiration of Black Being. Berlin: Sternberg Press."
  - "Eubanks, Virginia. 2017. Automating Inequality: How High-Tech Tools Profile, Police, and Punish the Poor. New York: St. Martin’s Press."
  - "Ovalle, A., Subramonian, A., Gautam, V., Gee, G., & Chang, K.-W. (2023). Factoring the Matrix of Domination : A Critical Review and Reimagination of Intersectionality in AI Fairness (arXiv:2303.17555). arXiv. https://doi.org/10.48550/arXiv.2303.17555"
